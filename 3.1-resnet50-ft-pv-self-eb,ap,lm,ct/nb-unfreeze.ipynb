{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset_path_plantvillage = '../datasets/plantvillage'\n",
    "dataset_path_self = '../datasets/self'\n",
    "dataset_path_validation = '../datasets/validation'\n",
    "\n",
    "dataset_paths = [dataset_path_plantvillage, dataset_path_self, dataset_path_validation]\n",
    "\n",
    "warmup_model_path = 'model-warmup.h5'\n",
    "model_save_path = 'model-final.h5'\n",
    "checkpoint_path = 'checkpoints-finetuning.hdf5'\n",
    "\n",
    "input_width = 224\n",
    "input_height = 224\n",
    "input_depth = 3\n",
    "\n",
    "# --------------------------------------------------\n",
    "num_of_epochs = 100\n",
    "start_epoch = 0\n",
    "# --------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select training classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_paths_training = ['../datasets/self/___Early_blight', '../datasets/self/___Appids', '../datasets/self/___Leaf_miner', '../datasets/self/___Curly_top_virus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO]: Processed 500/4849\n",
      "[INFO]: Processed 1000/4849\n",
      "[INFO]: Processed 1500/4849\n",
      "[INFO]: Processed 2000/4849\n",
      "[INFO]: Processed 2500/4849\n",
      "[INFO]: Processed 3000/4849\n",
      "[INFO]: Processed 3500/4849\n",
      "[INFO]: Processed 4000/4849\n",
      "[INFO]: Processed 4500/4849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.image   import ImageDataGenerator\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras .applications import VGG16\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.preprocessors.resize_image_preprocessor import resizeImagePreprocessor\n",
    "from utils.preprocessors.img_to_array_preprocessor import ImgToArrayPreprocessor\n",
    "from utils.io.dataset_loader import DatasetLoader\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(   rotation_range=30,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "\n",
    "def load_datasets(path_list):\n",
    "    # Load image paths\n",
    "    image_paths = []\n",
    "    print(\"[INFO] loading images...\")\n",
    "    for path in path_list:\n",
    "        image_paths.extend(list(paths.list_images(path)))\n",
    "        \n",
    "    # Get unique classnames\n",
    "\n",
    "    class_names = [pt.split(os.path.sep)[-2] for pt in image_paths]\n",
    "    class_names = [str(x) for x in np.unique(class_names)]\n",
    "\n",
    "    # Initial image preprocessing\n",
    "    aap = resizeImagePreprocessor(input_width, input_height)\n",
    "    iap= ImgToArrayPreprocessor()\n",
    "\n",
    "    #Load image data and perform image data preprocessing\n",
    "    dl = DatasetLoader(preprocessors=[aap,iap])\n",
    "    (data,labels)  = dl.load(image_paths,verbose=500)\n",
    "    data = data.astype(\"float\") / 255.0\n",
    "\n",
    "\n",
    "    # train test split\n",
    "    (train_x,test_x,train_y,test_y) = train_test_split(data,labels,test_size=0.25,random_state=42)\n",
    "\n",
    "    # convert the labels from integers to vectors\n",
    "    train_y = LabelBinarizer().fit_transform(train_y)\n",
    "    test_y = LabelBinarizer().fit_transform(test_y)\n",
    "    \n",
    "    return (train_x,test_x,train_y,test_y, class_names)\n",
    "\n",
    "(train_x,test_x,train_y,test_y, class_names) = load_datasets(class_paths_training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print(tf.__version__)\n",
    "\n",
    "\n",
    "# model = tf.keras.models.load_model(\n",
    "#     warmup_model_path,\n",
    "#     custom_objects=None,\n",
    "#     compile=False\n",
    "# )\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(warmup_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print index of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]     0\tinput_2                       InputLayer\n",
      "[INFO]     1\tconv1_pad                     ZeroPadding2D\n",
      "[INFO]     2\tconv1                         Conv2D\n",
      "[INFO]     3\tbn_conv1                      BatchNormalization\n",
      "[INFO]     4\tactivation_50                 Activation\n",
      "[INFO]     5\tpool1_pad                     ZeroPadding2D\n",
      "[INFO]     6\tmax_pooling2d_2               MaxPooling2D\n",
      "[INFO]     7\tres2a_branch2a                Conv2D\n",
      "[INFO]     8\tbn2a_branch2a                 BatchNormalization\n",
      "[INFO]     9\tactivation_51                 Activation\n",
      "[INFO]    10\tres2a_branch2b                Conv2D\n",
      "[INFO]    11\tbn2a_branch2b                 BatchNormalization\n",
      "[INFO]    12\tactivation_52                 Activation\n",
      "[INFO]    13\tres2a_branch2c                Conv2D\n",
      "[INFO]    14\tres2a_branch1                 Conv2D\n",
      "[INFO]    15\tbn2a_branch2c                 BatchNormalization\n",
      "[INFO]    16\tbn2a_branch1                  BatchNormalization\n",
      "[INFO]    17\tadd_17                        Add\n",
      "[INFO]    18\tactivation_53                 Activation\n",
      "[INFO]    19\tres2b_branch2a                Conv2D\n",
      "[INFO]    20\tbn2b_branch2a                 BatchNormalization\n",
      "[INFO]    21\tactivation_54                 Activation\n",
      "[INFO]    22\tres2b_branch2b                Conv2D\n",
      "[INFO]    23\tbn2b_branch2b                 BatchNormalization\n",
      "[INFO]    24\tactivation_55                 Activation\n",
      "[INFO]    25\tres2b_branch2c                Conv2D\n",
      "[INFO]    26\tbn2b_branch2c                 BatchNormalization\n",
      "[INFO]    27\tadd_18                        Add\n",
      "[INFO]    28\tactivation_56                 Activation\n",
      "[INFO]    29\tres2c_branch2a                Conv2D\n",
      "[INFO]    30\tbn2c_branch2a                 BatchNormalization\n",
      "[INFO]    31\tactivation_57                 Activation\n",
      "[INFO]    32\tres2c_branch2b                Conv2D\n",
      "[INFO]    33\tbn2c_branch2b                 BatchNormalization\n",
      "[INFO]    34\tactivation_58                 Activation\n",
      "[INFO]    35\tres2c_branch2c                Conv2D\n",
      "[INFO]    36\tbn2c_branch2c                 BatchNormalization\n",
      "[INFO]    37\tadd_19                        Add\n",
      "[INFO]    38\tactivation_59                 Activation\n",
      "[INFO]    39\tres3a_branch2a                Conv2D\n",
      "[INFO]    40\tbn3a_branch2a                 BatchNormalization\n",
      "[INFO]    41\tactivation_60                 Activation\n",
      "[INFO]    42\tres3a_branch2b                Conv2D\n",
      "[INFO]    43\tbn3a_branch2b                 BatchNormalization\n",
      "[INFO]    44\tactivation_61                 Activation\n",
      "[INFO]    45\tres3a_branch2c                Conv2D\n",
      "[INFO]    46\tres3a_branch1                 Conv2D\n",
      "[INFO]    47\tbn3a_branch2c                 BatchNormalization\n",
      "[INFO]    48\tbn3a_branch1                  BatchNormalization\n",
      "[INFO]    49\tadd_20                        Add\n",
      "[INFO]    50\tactivation_62                 Activation\n",
      "[INFO]    51\tres3b_branch2a                Conv2D\n",
      "[INFO]    52\tbn3b_branch2a                 BatchNormalization\n",
      "[INFO]    53\tactivation_63                 Activation\n",
      "[INFO]    54\tres3b_branch2b                Conv2D\n",
      "[INFO]    55\tbn3b_branch2b                 BatchNormalization\n",
      "[INFO]    56\tactivation_64                 Activation\n",
      "[INFO]    57\tres3b_branch2c                Conv2D\n",
      "[INFO]    58\tbn3b_branch2c                 BatchNormalization\n",
      "[INFO]    59\tadd_21                        Add\n",
      "[INFO]    60\tactivation_65                 Activation\n",
      "[INFO]    61\tres3c_branch2a                Conv2D\n",
      "[INFO]    62\tbn3c_branch2a                 BatchNormalization\n",
      "[INFO]    63\tactivation_66                 Activation\n",
      "[INFO]    64\tres3c_branch2b                Conv2D\n",
      "[INFO]    65\tbn3c_branch2b                 BatchNormalization\n",
      "[INFO]    66\tactivation_67                 Activation\n",
      "[INFO]    67\tres3c_branch2c                Conv2D\n",
      "[INFO]    68\tbn3c_branch2c                 BatchNormalization\n",
      "[INFO]    69\tadd_22                        Add\n",
      "[INFO]    70\tactivation_68                 Activation\n",
      "[INFO]    71\tres3d_branch2a                Conv2D\n",
      "[INFO]    72\tbn3d_branch2a                 BatchNormalization\n",
      "[INFO]    73\tactivation_69                 Activation\n",
      "[INFO]    74\tres3d_branch2b                Conv2D\n",
      "[INFO]    75\tbn3d_branch2b                 BatchNormalization\n",
      "[INFO]    76\tactivation_70                 Activation\n",
      "[INFO]    77\tres3d_branch2c                Conv2D\n",
      "[INFO]    78\tbn3d_branch2c                 BatchNormalization\n",
      "[INFO]    79\tadd_23                        Add\n",
      "[INFO]    80\tactivation_71                 Activation\n",
      "[INFO]    81\tres4a_branch2a                Conv2D\n",
      "[INFO]    82\tbn4a_branch2a                 BatchNormalization\n",
      "[INFO]    83\tactivation_72                 Activation\n",
      "[INFO]    84\tres4a_branch2b                Conv2D\n",
      "[INFO]    85\tbn4a_branch2b                 BatchNormalization\n",
      "[INFO]    86\tactivation_73                 Activation\n",
      "[INFO]    87\tres4a_branch2c                Conv2D\n",
      "[INFO]    88\tres4a_branch1                 Conv2D\n",
      "[INFO]    89\tbn4a_branch2c                 BatchNormalization\n",
      "[INFO]    90\tbn4a_branch1                  BatchNormalization\n",
      "[INFO]    91\tadd_24                        Add\n",
      "[INFO]    92\tactivation_74                 Activation\n",
      "[INFO]    93\tres4b_branch2a                Conv2D\n",
      "[INFO]    94\tbn4b_branch2a                 BatchNormalization\n",
      "[INFO]    95\tactivation_75                 Activation\n",
      "[INFO]    96\tres4b_branch2b                Conv2D\n",
      "[INFO]    97\tbn4b_branch2b                 BatchNormalization\n",
      "[INFO]    98\tactivation_76                 Activation\n",
      "[INFO]    99\tres4b_branch2c                Conv2D\n",
      "[INFO]   100\tbn4b_branch2c                 BatchNormalization\n",
      "[INFO]   101\tadd_25                        Add\n",
      "[INFO]   102\tactivation_77                 Activation\n",
      "[INFO]   103\tres4c_branch2a                Conv2D\n",
      "[INFO]   104\tbn4c_branch2a                 BatchNormalization\n",
      "[INFO]   105\tactivation_78                 Activation\n",
      "[INFO]   106\tres4c_branch2b                Conv2D\n",
      "[INFO]   107\tbn4c_branch2b                 BatchNormalization\n",
      "[INFO]   108\tactivation_79                 Activation\n",
      "[INFO]   109\tres4c_branch2c                Conv2D\n",
      "[INFO]   110\tbn4c_branch2c                 BatchNormalization\n",
      "[INFO]   111\tadd_26                        Add\n",
      "[INFO]   112\tactivation_80                 Activation\n",
      "[INFO]   113\tres4d_branch2a                Conv2D\n",
      "[INFO]   114\tbn4d_branch2a                 BatchNormalization\n",
      "[INFO]   115\tactivation_81                 Activation\n",
      "[INFO]   116\tres4d_branch2b                Conv2D\n",
      "[INFO]   117\tbn4d_branch2b                 BatchNormalization\n",
      "[INFO]   118\tactivation_82                 Activation\n",
      "[INFO]   119\tres4d_branch2c                Conv2D\n",
      "[INFO]   120\tbn4d_branch2c                 BatchNormalization\n",
      "[INFO]   121\tadd_27                        Add\n",
      "[INFO]   122\tactivation_83                 Activation\n",
      "[INFO]   123\tres4e_branch2a                Conv2D\n",
      "[INFO]   124\tbn4e_branch2a                 BatchNormalization\n",
      "[INFO]   125\tactivation_84                 Activation\n",
      "[INFO]   126\tres4e_branch2b                Conv2D\n",
      "[INFO]   127\tbn4e_branch2b                 BatchNormalization\n",
      "[INFO]   128\tactivation_85                 Activation\n",
      "[INFO]   129\tres4e_branch2c                Conv2D\n",
      "[INFO]   130\tbn4e_branch2c                 BatchNormalization\n",
      "[INFO]   131\tadd_28                        Add\n",
      "[INFO]   132\tactivation_86                 Activation\n",
      "[INFO]   133\tres4f_branch2a                Conv2D\n",
      "[INFO]   134\tbn4f_branch2a                 BatchNormalization\n",
      "[INFO]   135\tactivation_87                 Activation\n",
      "[INFO]   136\tres4f_branch2b                Conv2D\n",
      "[INFO]   137\tbn4f_branch2b                 BatchNormalization\n",
      "[INFO]   138\tactivation_88                 Activation\n",
      "[INFO]   139\tres4f_branch2c                Conv2D\n",
      "[INFO]   140\tbn4f_branch2c                 BatchNormalization\n",
      "[INFO]   141\tadd_29                        Add\n",
      "[INFO]   142\tactivation_89                 Activation\n",
      "[INFO]   143\tres5a_branch2a                Conv2D\n",
      "[INFO]   144\tbn5a_branch2a                 BatchNormalization\n",
      "[INFO]   145\tactivation_90                 Activation\n",
      "[INFO]   146\tres5a_branch2b                Conv2D\n",
      "[INFO]   147\tbn5a_branch2b                 BatchNormalization\n",
      "[INFO]   148\tactivation_91                 Activation\n",
      "[INFO]   149\tres5a_branch2c                Conv2D\n",
      "[INFO]   150\tres5a_branch1                 Conv2D\n",
      "[INFO]   151\tbn5a_branch2c                 BatchNormalization\n",
      "[INFO]   152\tbn5a_branch1                  BatchNormalization\n",
      "[INFO]   153\tadd_30                        Add\n",
      "[INFO]   154\tactivation_92                 Activation\n",
      "[INFO]   155\tres5b_branch2a                Conv2D\n",
      "[INFO]   156\tbn5b_branch2a                 BatchNormalization\n",
      "[INFO]   157\tactivation_93                 Activation\n",
      "[INFO]   158\tres5b_branch2b                Conv2D\n",
      "[INFO]   159\tbn5b_branch2b                 BatchNormalization\n",
      "[INFO]   160\tactivation_94                 Activation\n",
      "[INFO]   161\tres5b_branch2c                Conv2D\n",
      "[INFO]   162\tbn5b_branch2c                 BatchNormalization\n",
      "[INFO]   163\tadd_31                        Add\n",
      "[INFO]   164\tactivation_95                 Activation\n",
      "[INFO]   165\tres5c_branch2a                Conv2D\n",
      "[INFO]   166\tbn5c_branch2a                 BatchNormalization\n",
      "[INFO]   167\tactivation_96                 Activation\n",
      "[INFO]   168\tres5c_branch2b                Conv2D\n",
      "[INFO]   169\tbn5c_branch2b                 BatchNormalization\n",
      "[INFO]   170\tactivation_97                 Activation\n",
      "[INFO]   171\tres5c_branch2c                Conv2D\n",
      "[INFO]   172\tbn5c_branch2c                 BatchNormalization\n",
      "[INFO]   173\tadd_32                        Add\n",
      "[INFO]   174\tactivation_98                 Activation\n",
      "[INFO]   175\tglobal_average_pooling2d_2    GlobalAveragePooling2D\n",
      "[INFO]   176\tdense_2                       Dense\n"
     ]
    }
   ],
   "source": [
    "# iterate for all layers in the network and print its' index value\n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(\"[INFO] {:5}\\t{:30}{}\".format(i, layer.name, layer.__class__.__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfreeze final CONV layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfrozen\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers[:155]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[155:]:\n",
    "   layer.trainable = True\n",
    "print('unfrozen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] re-compiling model ...\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "# Build the model from the new\n",
    "print(\"[INFO] re-compiling model ...\")\n",
    "opt = SGD(lr=0.0001, momentum=0.09)\n",
    "# Fine-tuning with a small learning rate\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = opt,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', mode='min', \n",
    "save_best_only=True, verbose=1)\n",
    "\n",
    "callbacks = [checkpoint]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load checkpoints if existing\n",
    "\n",
    "import os\n",
    "\n",
    "if(os.path.exists(checkpoint_path)):\n",
    "    model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.callbacks.monitor import Monitor\n",
    "import pathlib\n",
    "import json\n",
    "import os\n",
    "\n",
    "fig_path = \"plot\"\n",
    "json_path = \"values.json\"\n",
    "values_path = 'values.json'\n",
    "\n",
    "callbacks.append(Monitor(fig_path, json_path, start_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "113/113 [==============================] - 70s 621ms/step - loss: 0.0427 - accuracy: 0.9878 - val_loss: 2.2431 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.24312, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 2/100\n",
      "113/113 [==============================] - 62s 551ms/step - loss: 0.0461 - accuracy: 0.9836 - val_loss: 2.2269 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.24312 to 2.22691, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 3/100\n",
      "113/113 [==============================] - 61s 544ms/step - loss: 0.0485 - accuracy: 0.9850 - val_loss: 2.2688 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.22691\n",
      "Epoch 4/100\n",
      "113/113 [==============================] - 61s 538ms/step - loss: 0.0467 - accuracy: 0.9825 - val_loss: 2.2758 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 2.22691\n",
      "Epoch 5/100\n",
      "113/113 [==============================] - 62s 551ms/step - loss: 0.0458 - accuracy: 0.9825 - val_loss: 2.2821 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 2.22691\n",
      "Epoch 6/100\n",
      "113/113 [==============================] - 62s 545ms/step - loss: 0.0408 - accuracy: 0.9864 - val_loss: 2.2705 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.22691\n",
      "Epoch 7/100\n",
      "113/113 [==============================] - 62s 547ms/step - loss: 0.0400 - accuracy: 0.9850 - val_loss: 2.2793 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 2.22691\n",
      "Epoch 8/100\n",
      "113/113 [==============================] - 61s 540ms/step - loss: 0.0448 - accuracy: 0.9853 - val_loss: 2.2664 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 2.22691\n",
      "Epoch 9/100\n",
      "113/113 [==============================] - 62s 547ms/step - loss: 0.0469 - accuracy: 0.9836 - val_loss: 2.2568 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 2.22691\n",
      "Epoch 10/100\n",
      "113/113 [==============================] - 61s 543ms/step - loss: 0.0440 - accuracy: 0.9839 - val_loss: 2.2636 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 2.22691\n",
      "Epoch 11/100\n",
      "113/113 [==============================] - 61s 540ms/step - loss: 0.0397 - accuracy: 0.9878 - val_loss: 2.2503 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 2.22691\n",
      "Epoch 12/100\n",
      "113/113 [==============================] - 61s 536ms/step - loss: 0.0448 - accuracy: 0.9873 - val_loss: 2.2394 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 2.22691\n",
      "Epoch 13/100\n",
      "113/113 [==============================] - 61s 537ms/step - loss: 0.0471 - accuracy: 0.9825 - val_loss: 2.2406 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 2.22691\n",
      "Epoch 14/100\n",
      "113/113 [==============================] - 60s 530ms/step - loss: 0.0417 - accuracy: 0.9850 - val_loss: 2.2272 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 2.22691\n",
      "Epoch 15/100\n",
      "113/113 [==============================] - 60s 534ms/step - loss: 0.0440 - accuracy: 0.9858 - val_loss: 2.2341 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 2.22691\n",
      "Epoch 16/100\n",
      "113/113 [==============================] - 61s 538ms/step - loss: 0.0459 - accuracy: 0.9856 - val_loss: 2.2251 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.22691 to 2.22508, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 17/100\n",
      "113/113 [==============================] - 60s 532ms/step - loss: 0.0371 - accuracy: 0.9897 - val_loss: 2.2167 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00017: val_loss improved from 2.22508 to 2.21667, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 18/100\n",
      "113/113 [==============================] - 61s 536ms/step - loss: 0.0440 - accuracy: 0.9844 - val_loss: 2.2236 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 2.21667\n",
      "Epoch 19/100\n",
      "113/113 [==============================] - 61s 543ms/step - loss: 0.0322 - accuracy: 0.9883 - val_loss: 2.2300 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 2.21667\n",
      "Epoch 20/100\n",
      "113/113 [==============================] - 60s 532ms/step - loss: 0.0336 - accuracy: 0.9870 - val_loss: 2.2274 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 2.21667\n",
      "Epoch 21/100\n",
      "113/113 [==============================] - 60s 530ms/step - loss: 0.0392 - accuracy: 0.9875 - val_loss: 2.2231 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 2.21667\n",
      "Epoch 22/100\n",
      "113/113 [==============================] - 60s 535ms/step - loss: 0.0315 - accuracy: 0.9912 - val_loss: 2.2216 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 2.21667\n",
      "Epoch 23/100\n",
      "113/113 [==============================] - 60s 532ms/step - loss: 0.0402 - accuracy: 0.9881 - val_loss: 2.2208 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 2.21667\n",
      "Epoch 24/100\n",
      "113/113 [==============================] - 60s 533ms/step - loss: 0.0361 - accuracy: 0.9880 - val_loss: 2.2268 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 2.21667\n",
      "Epoch 25/100\n",
      "113/113 [==============================] - 60s 527ms/step - loss: 0.0421 - accuracy: 0.9847 - val_loss: 2.2190 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 2.21667\n",
      "Epoch 26/100\n",
      "113/113 [==============================] - 60s 529ms/step - loss: 0.0400 - accuracy: 0.9875 - val_loss: 2.2105 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00026: val_loss improved from 2.21667 to 2.21045, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 27/100\n",
      "113/113 [==============================] - 59s 521ms/step - loss: 0.0397 - accuracy: 0.9842 - val_loss: 2.2155 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 2.21045\n",
      "Epoch 28/100\n",
      "113/113 [==============================] - 59s 523ms/step - loss: 0.0343 - accuracy: 0.9883 - val_loss: 2.2242 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 2.21045\n",
      "Epoch 29/100\n",
      "113/113 [==============================] - 59s 520ms/step - loss: 0.0428 - accuracy: 0.9867 - val_loss: 2.2081 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00029: val_loss improved from 2.21045 to 2.20807, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 30/100\n",
      "113/113 [==============================] - 59s 522ms/step - loss: 0.0398 - accuracy: 0.9878 - val_loss: 2.2135 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 2.20807\n",
      "Epoch 31/100\n",
      "113/113 [==============================] - 59s 523ms/step - loss: 0.0366 - accuracy: 0.9873 - val_loss: 2.2086 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 2.20807\n",
      "Epoch 32/100\n",
      "113/113 [==============================] - 59s 518ms/step - loss: 0.0410 - accuracy: 0.9878 - val_loss: 2.2053 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00032: val_loss improved from 2.20807 to 2.20528, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 33/100\n",
      "113/113 [==============================] - 60s 531ms/step - loss: 0.0361 - accuracy: 0.9892 - val_loss: 2.2004 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00033: val_loss improved from 2.20528 to 2.20042, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 34/100\n",
      "113/113 [==============================] - 59s 523ms/step - loss: 0.0414 - accuracy: 0.9867 - val_loss: 2.2106 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 2.20042\n",
      "Epoch 35/100\n",
      "113/113 [==============================] - 59s 523ms/step - loss: 0.0369 - accuracy: 0.9875 - val_loss: 2.1839 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00035: val_loss improved from 2.20042 to 2.18393, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 36/100\n",
      "113/113 [==============================] - 59s 523ms/step - loss: 0.0415 - accuracy: 0.9864 - val_loss: 2.1873 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 2.18393\n",
      "Epoch 37/100\n",
      "113/113 [==============================] - 59s 521ms/step - loss: 0.0382 - accuracy: 0.9870 - val_loss: 2.1886 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 2.18393\n",
      "Epoch 38/100\n",
      "113/113 [==============================] - 59s 518ms/step - loss: 0.0334 - accuracy: 0.9900 - val_loss: 2.1937 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 2.18393\n",
      "Epoch 39/100\n",
      "113/113 [==============================] - 59s 523ms/step - loss: 0.0364 - accuracy: 0.9872 - val_loss: 2.1826 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00039: val_loss improved from 2.18393 to 2.18262, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 40/100\n",
      "113/113 [==============================] - 60s 527ms/step - loss: 0.0344 - accuracy: 0.9889 - val_loss: 2.2027 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 2.18262\n",
      "Epoch 41/100\n",
      "113/113 [==============================] - 59s 521ms/step - loss: 0.0391 - accuracy: 0.9858 - val_loss: 2.1854 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 2.18262\n",
      "Epoch 42/100\n",
      "113/113 [==============================] - 59s 521ms/step - loss: 0.0373 - accuracy: 0.9845 - val_loss: 2.1726 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00042: val_loss improved from 2.18262 to 2.17262, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 43/100\n",
      "113/113 [==============================] - 58s 514ms/step - loss: 0.0294 - accuracy: 0.9894 - val_loss: 2.1855 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 2.17262\n",
      "Epoch 44/100\n",
      "113/113 [==============================] - 58s 515ms/step - loss: 0.0355 - accuracy: 0.9881 - val_loss: 2.1767 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.17262\n",
      "Epoch 45/100\n",
      "113/113 [==============================] - 58s 511ms/step - loss: 0.0416 - accuracy: 0.9867 - val_loss: 2.1928 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 2.17262\n",
      "Epoch 46/100\n",
      "113/113 [==============================] - 58s 513ms/step - loss: 0.0319 - accuracy: 0.9886 - val_loss: 2.1838 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.17262\n",
      "Epoch 47/100\n",
      "113/113 [==============================] - 59s 522ms/step - loss: 0.0340 - accuracy: 0.9873 - val_loss: 2.1903 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.17262\n",
      "Epoch 48/100\n",
      "113/113 [==============================] - 59s 519ms/step - loss: 0.0333 - accuracy: 0.9880 - val_loss: 2.1830 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.17262\n",
      "Epoch 49/100\n",
      "113/113 [==============================] - 58s 513ms/step - loss: 0.0347 - accuracy: 0.9870 - val_loss: 2.1728 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.17262\n",
      "Epoch 50/100\n",
      "113/113 [==============================] - 58s 513ms/step - loss: 0.0374 - accuracy: 0.9850 - val_loss: 2.1870 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.17262\n",
      "Epoch 51/100\n",
      "113/113 [==============================] - 58s 513ms/step - loss: 0.0332 - accuracy: 0.9900 - val_loss: 2.1822 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.17262\n",
      "Epoch 52/100\n",
      "113/113 [==============================] - 59s 524ms/step - loss: 0.0345 - accuracy: 0.9883 - val_loss: 2.1906 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.17262\n",
      "Epoch 53/100\n",
      "113/113 [==============================] - 58s 517ms/step - loss: 0.0369 - accuracy: 0.9872 - val_loss: 2.1810 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.17262\n",
      "Epoch 54/100\n",
      "113/113 [==============================] - 58s 512ms/step - loss: 0.0297 - accuracy: 0.9905 - val_loss: 2.1901 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.17262\n",
      "Epoch 55/100\n",
      "113/113 [==============================] - 57s 507ms/step - loss: 0.0282 - accuracy: 0.9897 - val_loss: 2.1761 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 2.17262\n",
      "Epoch 56/100\n",
      "113/113 [==============================] - 57s 508ms/step - loss: 0.0297 - accuracy: 0.9892 - val_loss: 2.1678 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00056: val_loss improved from 2.17262 to 2.16784, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 57/100\n",
      "113/113 [==============================] - 58s 510ms/step - loss: 0.0272 - accuracy: 0.9911 - val_loss: 2.1728 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 2.16784\n",
      "Epoch 58/100\n",
      "113/113 [==============================] - 58s 513ms/step - loss: 0.0303 - accuracy: 0.9911 - val_loss: 2.1814 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.16784\n",
      "Epoch 59/100\n",
      "113/113 [==============================] - 58s 511ms/step - loss: 0.0427 - accuracy: 0.9834 - val_loss: 2.1729 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2.16784\n",
      "Epoch 60/100\n",
      "113/113 [==============================] - 57s 506ms/step - loss: 0.0368 - accuracy: 0.9866 - val_loss: 2.1834 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2.16784\n",
      "Epoch 61/100\n",
      "113/113 [==============================] - 58s 511ms/step - loss: 0.0336 - accuracy: 0.9892 - val_loss: 2.1706 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2.16784\n",
      "Epoch 62/100\n",
      "113/113 [==============================] - 58s 512ms/step - loss: 0.0374 - accuracy: 0.9892 - val_loss: 2.1779 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2.16784\n",
      "Epoch 63/100\n",
      "113/113 [==============================] - 57s 509ms/step - loss: 0.0331 - accuracy: 0.9897 - val_loss: 2.1721 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2.16784\n",
      "Epoch 64/100\n",
      "113/113 [==============================] - 57s 508ms/step - loss: 0.0352 - accuracy: 0.9895 - val_loss: 2.1553 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00064: val_loss improved from 2.16784 to 2.15527, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 65/100\n",
      "113/113 [==============================] - 57s 506ms/step - loss: 0.0295 - accuracy: 0.9914 - val_loss: 2.1573 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 2.15527\n",
      "Epoch 66/100\n",
      "113/113 [==============================] - 57s 505ms/step - loss: 0.0345 - accuracy: 0.9886 - val_loss: 2.1566 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.15527\n",
      "Epoch 67/100\n",
      "113/113 [==============================] - 57s 509ms/step - loss: 0.0428 - accuracy: 0.9831 - val_loss: 2.1685 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 2.15527\n",
      "Epoch 68/100\n",
      "113/113 [==============================] - 57s 500ms/step - loss: 0.0332 - accuracy: 0.9900 - val_loss: 2.1677 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 2.15527\n",
      "Epoch 69/100\n",
      "113/113 [==============================] - 58s 510ms/step - loss: 0.0316 - accuracy: 0.9895 - val_loss: 2.1710 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.15527\n",
      "Epoch 70/100\n",
      "113/113 [==============================] - 58s 512ms/step - loss: 0.0358 - accuracy: 0.9892 - val_loss: 2.1683 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.15527\n",
      "Epoch 71/100\n",
      "113/113 [==============================] - 57s 504ms/step - loss: 0.0310 - accuracy: 0.9886 - val_loss: 2.1616 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2.15527\n",
      "Epoch 72/100\n",
      "113/113 [==============================] - 56s 497ms/step - loss: 0.0370 - accuracy: 0.9883 - val_loss: 2.1616 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2.15527\n",
      "Epoch 73/100\n",
      "113/113 [==============================] - 57s 507ms/step - loss: 0.0309 - accuracy: 0.9891 - val_loss: 2.1673 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2.15527\n",
      "Epoch 74/100\n",
      "113/113 [==============================] - 56s 497ms/step - loss: 0.0362 - accuracy: 0.9900 - val_loss: 2.1819 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 2.15527\n",
      "Epoch 75/100\n",
      "113/113 [==============================] - 56s 499ms/step - loss: 0.0325 - accuracy: 0.9897 - val_loss: 2.1729 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2.15527\n",
      "Epoch 76/100\n",
      "113/113 [==============================] - 56s 496ms/step - loss: 0.0349 - accuracy: 0.9886 - val_loss: 2.1719 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2.15527\n",
      "Epoch 77/100\n",
      "113/113 [==============================] - 56s 492ms/step - loss: 0.0321 - accuracy: 0.9897 - val_loss: 2.1708 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 2.15527\n",
      "Epoch 78/100\n",
      "113/113 [==============================] - 56s 492ms/step - loss: 0.0354 - accuracy: 0.9898 - val_loss: 2.1746 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2.15527\n",
      "Epoch 79/100\n",
      "113/113 [==============================] - 56s 493ms/step - loss: 0.0286 - accuracy: 0.9908 - val_loss: 2.1820 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 2.15527\n",
      "Epoch 80/100\n",
      "113/113 [==============================] - 56s 495ms/step - loss: 0.0301 - accuracy: 0.9906 - val_loss: 2.1757 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 2.15527\n",
      "Epoch 81/100\n",
      "113/113 [==============================] - 56s 498ms/step - loss: 0.0295 - accuracy: 0.9897 - val_loss: 2.1704 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 2.15527\n",
      "Epoch 82/100\n",
      "113/113 [==============================] - 56s 493ms/step - loss: 0.0354 - accuracy: 0.9866 - val_loss: 2.1826 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 2.15527\n",
      "Epoch 83/100\n",
      "113/113 [==============================] - 56s 495ms/step - loss: 0.0313 - accuracy: 0.9889 - val_loss: 2.1709 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 2.15527\n",
      "Epoch 84/100\n",
      "113/113 [==============================] - 56s 493ms/step - loss: 0.0345 - accuracy: 0.9881 - val_loss: 2.1612 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 2.15527\n",
      "Epoch 85/100\n",
      "113/113 [==============================] - 55s 490ms/step - loss: 0.0274 - accuracy: 0.9911 - val_loss: 2.1599 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 2.15527\n",
      "Epoch 86/100\n",
      "113/113 [==============================] - 55s 489ms/step - loss: 0.0280 - accuracy: 0.9916 - val_loss: 2.1606 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 2.15527\n",
      "Epoch 87/100\n",
      "113/113 [==============================] - 56s 496ms/step - loss: 0.0290 - accuracy: 0.9920 - val_loss: 2.1637 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 2.15527\n",
      "Epoch 88/100\n",
      "113/113 [==============================] - 55s 489ms/step - loss: 0.0420 - accuracy: 0.9856 - val_loss: 2.1609 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 2.15527\n",
      "Epoch 89/100\n",
      "113/113 [==============================] - 55s 491ms/step - loss: 0.0334 - accuracy: 0.9908 - val_loss: 2.1486 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00089: val_loss improved from 2.15527 to 2.14861, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 90/100\n",
      "113/113 [==============================] - 56s 496ms/step - loss: 0.0400 - accuracy: 0.9856 - val_loss: 2.1547 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 2.14861\n",
      "Epoch 91/100\n",
      "113/113 [==============================] - 57s 502ms/step - loss: 0.0357 - accuracy: 0.9883 - val_loss: 2.1669 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 2.14861\n",
      "Epoch 92/100\n",
      "113/113 [==============================] - 57s 507ms/step - loss: 0.0282 - accuracy: 0.9889 - val_loss: 2.1508 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 2.14861\n",
      "Epoch 93/100\n",
      "113/113 [==============================] - 56s 498ms/step - loss: 0.0368 - accuracy: 0.9878 - val_loss: 2.1627 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 2.14861\n",
      "Epoch 94/100\n",
      "113/113 [==============================] - 58s 510ms/step - loss: 0.0329 - accuracy: 0.9883 - val_loss: 2.1600 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 2.14861\n",
      "Epoch 95/100\n",
      "113/113 [==============================] - 57s 501ms/step - loss: 0.0259 - accuracy: 0.9917 - val_loss: 2.1583 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 2.14861\n",
      "Epoch 96/100\n",
      "113/113 [==============================] - 56s 499ms/step - loss: 0.0358 - accuracy: 0.9881 - val_loss: 2.1614 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 2.14861\n",
      "Epoch 97/100\n",
      "113/113 [==============================] - 57s 506ms/step - loss: 0.0313 - accuracy: 0.9889 - val_loss: 2.1666 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 2.14861\n",
      "Epoch 98/100\n",
      "113/113 [==============================] - 58s 511ms/step - loss: 0.0285 - accuracy: 0.9920 - val_loss: 2.1702 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 2.14861\n",
      "Epoch 99/100\n",
      "113/113 [==============================] - 57s 506ms/step - loss: 0.0303 - accuracy: 0.9911 - val_loss: 2.1546 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 2.14861\n",
      "Epoch 100/100\n",
      "113/113 [==============================] - 57s 509ms/step - loss: 0.0336 - accuracy: 0.9889 - val_loss: 2.1680 - val_accuracy: 0.1171\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 2.14861\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 100\n",
    "\n",
    "H = model.fit_generator(\n",
    "    aug.flow(train_x,train_y, batch_size = 32),\n",
    "             validation_data = (test_x,test_y),\n",
    "             epochs=num_of_epochs,\n",
    "             steps_per_epoch = len(train_x) //32,\n",
    "             verbose = 1,\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating after initialization...\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         ___Appids     0.0000    0.0000    0.0000       242\n",
      "___Curly_top_virus     0.1171    1.0000    0.2096       142\n",
      "   ___Early_blight     0.0000    0.0000    0.0000       384\n",
      "     ___Leaf_miner     0.0000    0.0000    0.0000       445\n",
      "\n",
      "       avg / total     0.0137    0.1171    0.0245      1213\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"[INFO] evaluating after initialization...\")\n",
    "predictions = model.predict(test_x,batch_size=batch_size)\n",
    "\n",
    "print(classification_report(test_y.argmax(axis =1),\n",
    "                            predictions.argmax(axis =1),\n",
    "                            target_names=class_names, \n",
    "                            digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_paths_validation = ['../datasets/validation/___Early_blight', '../datasets/validation/___Appids', '../datasets/validation/___Leaf_miner', '../datasets/self/___Curly_top_virus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO]: Processed 500/614\n",
      "[INFO] evaluating with validation set...\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         ___Appids     0.0000    0.0000    0.0000         9\n",
      "___Curly_top_virus     0.7662    1.0000    0.8676       118\n",
      "   ___Early_blight     0.0000    0.0000    0.0000        12\n",
      "     ___Leaf_miner     0.0000    0.0000    0.0000        15\n",
      "\n",
      "       avg / total     0.5871    0.7662    0.6648       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(train_x,test_x,train_y,test_y, class_names) = load_datasets(class_paths_validation)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"[INFO] evaluating with validation set...\")\n",
    "predictions = model.predict(test_x,batch_size=batch_size)\n",
    "\n",
    "print(classification_report(test_y.argmax(axis =1),\n",
    "                            predictions.argmax(axis =1),\n",
    "                            target_names=class_names, \n",
    "                            digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset_path_plantvillage = '../datasets/plantvillage'\n",
    "dataset_path_self = '../datasets/self'\n",
    "dataset_path_validation = '../datasets/validation'\n",
    "\n",
    "dataset_paths = [dataset_path_plantvillage, dataset_path_self, dataset_path_validation]\n",
    "\n",
    "warmup_model_path = 'model-warmup.h5'\n",
    "model_save_path = 'model-final.h5'\n",
    "checkpoint_path = 'checkpoints-finetuning.hdf5'\n",
    "\n",
    "input_width = 224\n",
    "input_height = 224\n",
    "input_depth = 3\n",
    "\n",
    "# --------------------------------------------------\n",
    "<<<<<<< local\n",
    "num_of_epochs = 20\n",
    "start_epoch = 0\n",
    "=======\n",
    "num_of_epochs = 30\n",
    "start_epoch =  151 - 50 - 36\n",
    ">>>>>>> remote\n",
    "# --------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select training classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_paths_training = ['../datasets/self/___Early_blight', '../datasets/self/___Appids', '../datasets/self/___Leaf_miner', '../datasets/self/___Curly_top_virus']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO]: Processed 500/4401\n",
      "[INFO]: Processed 1000/4401\n",
      "[INFO]: Processed 1500/4401\n",
      "[INFO]: Processed 2000/4401\n",
      "[INFO]: Processed 2500/4401\n",
      "[INFO]: Processed 3000/4401\n",
      "[INFO]: Processed 3500/4401\n",
      "[INFO]: Processed 4000/4401\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.image   import ImageDataGenerator\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras .applications import VGG16\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.preprocessors.resize_image_preprocessor import resizeImagePreprocessor\n",
    "from utils.preprocessors.img_to_array_preprocessor import ImgToArrayPreprocessor\n",
    "from utils.io.dataset_loader import DatasetLoader\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(   rotation_range=30,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "\n",
    "def load_datasets(path_list):\n",
    "    # Load image paths\n",
    "    image_paths = []\n",
    "    print(\"[INFO] loading images...\")\n",
    "    for path in path_list:\n",
    "        image_paths.extend(list(paths.list_images(path)))\n",
    "        \n",
    "    # Get unique classnames\n",
    "\n",
    "    class_names = [pt.split(os.path.sep)[-2] for pt in image_paths]\n",
    "    class_names = [str(x) for x in np.unique(class_names)]\n",
    "\n",
    "    # Initial image preprocessing\n",
    "    aap = resizeImagePreprocessor(input_width, input_height)\n",
    "    iap= ImgToArrayPreprocessor()\n",
    "\n",
    "    #Load image data and perform image data preprocessing\n",
    "    dl = DatasetLoader(preprocessors=[aap,iap])\n",
    "    (data,labels)  = dl.load(image_paths,verbose=500)\n",
    "    data = data.astype(\"float\") / 255.0\n",
    "\n",
    "\n",
    "    # train test split\n",
    "    (train_x,test_x,train_y,test_y) = train_test_split(data,labels,test_size=0.25,random_state=42)\n",
    "\n",
    "    # convert the labels from integers to vectors\n",
    "    train_y = LabelBinarizer().fit_transform(train_y)\n",
    "    test_y = LabelBinarizer().fit_transform(test_y)\n",
    "    \n",
    "    return (train_x,test_x,train_y,test_y, class_names)\n",
    "\n",
    "(train_x,test_x,train_y,test_y, class_names) = load_datasets(class_paths_training)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8ea1667e38c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarmup_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'write'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode)\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_path_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# print(tf.__version__)\n",
    "\n",
    "\n",
    "# model = tf.keras.models.load_model(\n",
    "#     warmup_model_path,\n",
    "#     custom_objects=None,\n",
    "#     compile=False\n",
    "# )\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(warmup_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print index of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate for all layers in the network and print its' index value\n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(\"[INFO] {:5}\\t{:30}{}\".format(i, layer.name, layer.__class__.__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfreeze final CONV layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:7]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[7:]:\n",
    "   layer.trainable = True\n",
    "print('unfrozen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "# Build the model from the new\n",
    "print(\"[INFO] re-compiling model ...\")\n",
    "opt = SGD(lr=0.0001, momentum=0.09)\n",
    "# Fine-tuning with a small learning rate\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = opt,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', mode='min', \n",
    "save_best_only=True, verbose=1)\n",
    "\n",
    "callbacks = [checkpoint]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load checkpoints if existing\n",
    "\n",
    "import os\n",
    "\n",
    "if(os.path.exists(checkpoint_path)):\n",
    "    model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.callbacks.monitor import Monitor\n",
    "import pathlib\n",
    "import json\n",
    "import os\n",
    "\n",
    "fig_path = \"plot\"\n",
    "json_path = \"values.json\"\n",
    "values_path = 'values.json'\n",
    "\n",
    "callbacks.append(Monitor(fig_path, json_path, start_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "103/103 [==============================] - 74s 714ms/step - loss: 0.0686 - accuracy: 0.9767 - val_loss: 0.0520 - val_accuracy: 0.9918\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05204, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 2/30\n",
      "103/103 [==============================] - 64s 623ms/step - loss: 0.0826 - accuracy: 0.9746 - val_loss: 0.0625 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.05204\n",
      "Epoch 3/30\n",
      "103/103 [==============================] - 64s 626ms/step - loss: 0.0769 - accuracy: 0.9737 - val_loss: 0.0518 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.05204 to 0.05177, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 4/30\n",
      "103/103 [==============================] - 64s 622ms/step - loss: 0.0820 - accuracy: 0.9758 - val_loss: 0.0518 - val_accuracy: 0.9909\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.05177\n",
      "Epoch 5/30\n",
      "103/103 [==============================] - 64s 623ms/step - loss: 0.0701 - accuracy: 0.9816 - val_loss: 0.0497 - val_accuracy: 0.9909\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.05177 to 0.04970, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 6/30\n",
      "103/103 [==============================] - 65s 629ms/step - loss: 0.0816 - accuracy: 0.9736 - val_loss: 0.0503 - val_accuracy: 0.9891\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.04970\n",
      "Epoch 7/30\n",
      "103/103 [==============================] - 64s 624ms/step - loss: 0.0744 - accuracy: 0.9738 - val_loss: 0.0516 - val_accuracy: 0.9873\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.04970\n",
      "Epoch 8/30\n",
      "103/103 [==============================] - 64s 624ms/step - loss: 0.0696 - accuracy: 0.9823 - val_loss: 0.0554 - val_accuracy: 0.9846\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.04970\n",
      "Epoch 9/30\n",
      "103/103 [==============================] - 64s 624ms/step - loss: 0.0569 - accuracy: 0.9838 - val_loss: 0.0592 - val_accuracy: 0.9864\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.04970\n",
      "Epoch 10/30\n",
      "103/103 [==============================] - 64s 625ms/step - loss: 0.0734 - accuracy: 0.9722 - val_loss: 0.0517 - val_accuracy: 0.9891\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.04970\n",
      "Epoch 11/30\n",
      "103/103 [==============================] - 64s 625ms/step - loss: 0.0782 - accuracy: 0.9737 - val_loss: 0.0581 - val_accuracy: 0.9864\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.04970\n",
      "Epoch 12/30\n",
      "103/103 [==============================] - 64s 625ms/step - loss: 0.0695 - accuracy: 0.9813 - val_loss: 0.0748 - val_accuracy: 0.9791\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.04970\n",
      "Epoch 13/30\n",
      "103/103 [==============================] - 64s 625ms/step - loss: 0.0863 - accuracy: 0.9752 - val_loss: 0.0603 - val_accuracy: 0.9846\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.04970\n",
      "Epoch 14/30\n",
      "103/103 [==============================] - 64s 624ms/step - loss: 0.0741 - accuracy: 0.9792 - val_loss: 0.0553 - val_accuracy: 0.9909\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.04970\n",
      "Epoch 15/30\n",
      "103/103 [==============================] - 64s 624ms/step - loss: 0.0775 - accuracy: 0.9761 - val_loss: 0.0616 - val_accuracy: 0.9918\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.04970\n",
      "Epoch 16/30\n",
      "103/103 [==============================] - 65s 628ms/step - loss: 0.0723 - accuracy: 0.9763 - val_loss: 0.0660 - val_accuracy: 0.9864\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.04970\n",
      "Epoch 17/30\n",
      "103/103 [==============================] - 64s 622ms/step - loss: 0.0936 - accuracy: 0.9688 - val_loss: 0.0538 - val_accuracy: 0.9900\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.04970\n",
      "Epoch 18/30\n",
      "103/103 [==============================] - 64s 624ms/step - loss: 0.0655 - accuracy: 0.9795 - val_loss: 0.0715 - val_accuracy: 0.9818\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.04970\n",
      "Epoch 19/30\n",
      "103/103 [==============================] - 65s 628ms/step - loss: 0.0821 - accuracy: 0.9742 - val_loss: 0.0632 - val_accuracy: 0.9909\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.04970\n",
      "Epoch 20/30\n",
      "103/103 [==============================] - 64s 620ms/step - loss: 0.0846 - accuracy: 0.9725 - val_loss: 0.0727 - val_accuracy: 0.9855\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.04970\n",
      "Epoch 21/30\n",
      "103/103 [==============================] - 65s 628ms/step - loss: 0.0727 - accuracy: 0.9797 - val_loss: 0.0469 - val_accuracy: 0.9909\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.04970 to 0.04692, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 22/30\n",
      "103/103 [==============================] - 64s 622ms/step - loss: 0.0612 - accuracy: 0.9821 - val_loss: 0.0386 - val_accuracy: 0.9918\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.04692 to 0.03861, saving model to checkpoints-finetuning.hdf5\n",
      "Epoch 23/30\n",
      "103/103 [==============================] - 64s 624ms/step - loss: 0.0669 - accuracy: 0.9798 - val_loss: 0.0681 - val_accuracy: 0.9882\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.03861\n",
      "Epoch 24/30\n",
      "103/103 [==============================] - 64s 624ms/step - loss: 0.0726 - accuracy: 0.9777 - val_loss: 0.0449 - val_accuracy: 0.9909\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.03861\n",
      "Epoch 25/30\n",
      "103/103 [==============================] - 65s 628ms/step - loss: 0.0681 - accuracy: 0.9797 - val_loss: 0.0466 - val_accuracy: 0.9918\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.03861\n",
      "Epoch 26/30\n",
      "103/103 [==============================] - 64s 620ms/step - loss: 0.0730 - accuracy: 0.9759 - val_loss: 0.0487 - val_accuracy: 0.9909\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.03861\n",
      "Epoch 27/30\n",
      "103/103 [==============================] - 65s 628ms/step - loss: 0.0648 - accuracy: 0.9821 - val_loss: 0.0387 - val_accuracy: 0.9927\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.03861\n",
      "Epoch 28/30\n",
      "103/103 [==============================] - 64s 625ms/step - loss: 0.0671 - accuracy: 0.9850 - val_loss: 0.0484 - val_accuracy: 0.9927\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.03861\n",
      "Epoch 29/30\n",
      "103/103 [==============================] - 64s 625ms/step - loss: 0.0651 - accuracy: 0.9856 - val_loss: 0.0401 - val_accuracy: 0.9918\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.03861\n",
      "Epoch 30/30\n",
      "103/103 [==============================] - 64s 620ms/step - loss: 0.0679 - accuracy: 0.9799 - val_loss: 0.0477 - val_accuracy: 0.9927\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.03861\n"
     ]
    }
   ],
   "source": [
    "H = model.fit_generator(\n",
    "    aug.flow(train_x,train_y, batch_size = 32),\n",
    "             validation_data = (test_x,test_y),\n",
    "             epochs=num_of_epochs,\n",
    "             steps_per_epoch = len(train_x) //32,\n",
    "             verbose = 1,\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local <removed>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating after initialization...\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         ___Appids     0.9878    1.0000    0.9939       243\n",
      "___Curly_top_virus     0.9817    0.9817    0.9817       109\n",
      "   ___Early_blight     0.9948    0.9897    0.9922       388\n",
      "     ___Leaf_miner     0.9972    0.9945    0.9958       361\n",
      "\n",
      "       avg / total     0.9928    0.9927    0.9927      1101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>> remote <modified: >\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"[INFO] evaluating after initialization...\")\n",
    "predictions = model.predict(test_x,batch_size=batch_size)\n",
    "\n",
    "print(classification_report(test_y.argmax(axis =1),\n",
    "                            predictions.argmax(axis =1),\n",
    "                            target_names=class_names, \n",
    "                            digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate with validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_paths_validation = ['../datasets/validation/___Early_blight', '../datasets/validation/___Appids', '../datasets/validation/___Leaf_miner', '../datasets/self/___Curly_top_virus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local <removed>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO]: Processed 500/599\n",
      "[INFO] evaluating with validation set...\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         ___Appids     1.0000    0.6000    0.7500         5\n",
      "___Curly_top_virus     0.9739    1.0000    0.9868       112\n",
      "   ___Early_blight     0.7059    1.0000    0.8276        12\n",
      "     ___Leaf_miner     1.0000    0.7143    0.8333        21\n",
      "\n",
      "       avg / total     0.9570    0.9467    0.9447       150\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>> remote <modified: >\n"
     ]
    }
   ],
   "source": [
    "(train_x,test_x,train_y,test_y, class_names) = load_datasets(class_paths_validation)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"[INFO] evaluating with validation set...\")\n",
    "predictions = model.predict(test_x,batch_size=batch_size)\n",
    "\n",
    "print(classification_report(test_y.argmax(axis =1),\n",
    "                            predictions.argmax(axis =1),\n",
    "                            target_names=class_names, \n",
    "                            digits=4))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

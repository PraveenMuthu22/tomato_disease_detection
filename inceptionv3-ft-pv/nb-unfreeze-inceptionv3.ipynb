{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes:  ['Tomato___Tomato_mosaic_virus', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Bacterial_spot', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___healthy', 'Tomato___Septoria_leaf_spot', 'Tomato___Two-spotted_spider_mite']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "warmup_model_path = 'model-warmup.h5'\n",
    "model_save_path = 'model-final.h5'\n",
    "dataset_name = 'tomato-dataset'\n",
    "dataset_path = '../datasets/' + dataset_name\n",
    "checkpoint_path = 'checkpoints-finetuning.hdf5'\n",
    "input_width = 299\n",
    "input_height = 299\n",
    "input_depth = 3\n",
    "\n",
    "batch_size = 32\n",
    "num_of_epochs = 100\n",
    "\n",
    "# Get classes\n",
    "import os\n",
    "import re\n",
    "classes = os.listdir(dataset_path)\n",
    "class_names = []\n",
    "\n",
    "for i in classes:\n",
    "    if(re.search(\"Tomato___\", i)):\n",
    "        class_names.append(i)\n",
    "    \n",
    "print('Classes: ', class_names)\n",
    "print(len(class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# print(tf.__version__)\n",
    "\n",
    "\n",
    "# model = tf.keras.models.load_model(\n",
    "#     warmup_model_path,\n",
    "#     custom_objects=None,\n",
    "#     compile=False\n",
    "# )\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(warmup_model_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print index of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 0\tInputLayer\n",
      "[INFO] 1\tConv2D\n",
      "[INFO] 2\tBatchNormalization\n",
      "[INFO] 3\tActivation\n",
      "[INFO] 4\tConv2D\n",
      "[INFO] 5\tBatchNormalization\n",
      "[INFO] 6\tActivation\n",
      "[INFO] 7\tConv2D\n",
      "[INFO] 8\tBatchNormalization\n",
      "[INFO] 9\tActivation\n",
      "[INFO] 10\tMaxPooling2D\n",
      "[INFO] 11\tConv2D\n",
      "[INFO] 12\tBatchNormalization\n",
      "[INFO] 13\tActivation\n",
      "[INFO] 14\tConv2D\n",
      "[INFO] 15\tBatchNormalization\n",
      "[INFO] 16\tActivation\n",
      "[INFO] 17\tMaxPooling2D\n",
      "[INFO] 18\tConv2D\n",
      "[INFO] 19\tBatchNormalization\n",
      "[INFO] 20\tActivation\n",
      "[INFO] 21\tConv2D\n",
      "[INFO] 22\tConv2D\n",
      "[INFO] 23\tBatchNormalization\n",
      "[INFO] 24\tBatchNormalization\n",
      "[INFO] 25\tActivation\n",
      "[INFO] 26\tActivation\n",
      "[INFO] 27\tAveragePooling2D\n",
      "[INFO] 28\tConv2D\n",
      "[INFO] 29\tConv2D\n",
      "[INFO] 30\tConv2D\n",
      "[INFO] 31\tConv2D\n",
      "[INFO] 32\tBatchNormalization\n",
      "[INFO] 33\tBatchNormalization\n",
      "[INFO] 34\tBatchNormalization\n",
      "[INFO] 35\tBatchNormalization\n",
      "[INFO] 36\tActivation\n",
      "[INFO] 37\tActivation\n",
      "[INFO] 38\tActivation\n",
      "[INFO] 39\tActivation\n",
      "[INFO] 40\tConcatenate\n",
      "[INFO] 41\tConv2D\n",
      "[INFO] 42\tBatchNormalization\n",
      "[INFO] 43\tActivation\n",
      "[INFO] 44\tConv2D\n",
      "[INFO] 45\tConv2D\n",
      "[INFO] 46\tBatchNormalization\n",
      "[INFO] 47\tBatchNormalization\n",
      "[INFO] 48\tActivation\n",
      "[INFO] 49\tActivation\n",
      "[INFO] 50\tAveragePooling2D\n",
      "[INFO] 51\tConv2D\n",
      "[INFO] 52\tConv2D\n",
      "[INFO] 53\tConv2D\n",
      "[INFO] 54\tConv2D\n",
      "[INFO] 55\tBatchNormalization\n",
      "[INFO] 56\tBatchNormalization\n",
      "[INFO] 57\tBatchNormalization\n",
      "[INFO] 58\tBatchNormalization\n",
      "[INFO] 59\tActivation\n",
      "[INFO] 60\tActivation\n",
      "[INFO] 61\tActivation\n",
      "[INFO] 62\tActivation\n",
      "[INFO] 63\tConcatenate\n",
      "[INFO] 64\tConv2D\n",
      "[INFO] 65\tBatchNormalization\n",
      "[INFO] 66\tActivation\n",
      "[INFO] 67\tConv2D\n",
      "[INFO] 68\tConv2D\n",
      "[INFO] 69\tBatchNormalization\n",
      "[INFO] 70\tBatchNormalization\n",
      "[INFO] 71\tActivation\n",
      "[INFO] 72\tActivation\n",
      "[INFO] 73\tAveragePooling2D\n",
      "[INFO] 74\tConv2D\n",
      "[INFO] 75\tConv2D\n",
      "[INFO] 76\tConv2D\n",
      "[INFO] 77\tConv2D\n",
      "[INFO] 78\tBatchNormalization\n",
      "[INFO] 79\tBatchNormalization\n",
      "[INFO] 80\tBatchNormalization\n",
      "[INFO] 81\tBatchNormalization\n",
      "[INFO] 82\tActivation\n",
      "[INFO] 83\tActivation\n",
      "[INFO] 84\tActivation\n",
      "[INFO] 85\tActivation\n",
      "[INFO] 86\tConcatenate\n",
      "[INFO] 87\tConv2D\n",
      "[INFO] 88\tBatchNormalization\n",
      "[INFO] 89\tActivation\n",
      "[INFO] 90\tConv2D\n",
      "[INFO] 91\tBatchNormalization\n",
      "[INFO] 92\tActivation\n",
      "[INFO] 93\tConv2D\n",
      "[INFO] 94\tConv2D\n",
      "[INFO] 95\tBatchNormalization\n",
      "[INFO] 96\tBatchNormalization\n",
      "[INFO] 97\tActivation\n",
      "[INFO] 98\tActivation\n",
      "[INFO] 99\tMaxPooling2D\n",
      "[INFO] 100\tConcatenate\n",
      "[INFO] 101\tConv2D\n",
      "[INFO] 102\tBatchNormalization\n",
      "[INFO] 103\tActivation\n",
      "[INFO] 104\tConv2D\n",
      "[INFO] 105\tBatchNormalization\n",
      "[INFO] 106\tActivation\n",
      "[INFO] 107\tConv2D\n",
      "[INFO] 108\tConv2D\n",
      "[INFO] 109\tBatchNormalization\n",
      "[INFO] 110\tBatchNormalization\n",
      "[INFO] 111\tActivation\n",
      "[INFO] 112\tActivation\n",
      "[INFO] 113\tConv2D\n",
      "[INFO] 114\tConv2D\n",
      "[INFO] 115\tBatchNormalization\n",
      "[INFO] 116\tBatchNormalization\n",
      "[INFO] 117\tActivation\n",
      "[INFO] 118\tActivation\n",
      "[INFO] 119\tAveragePooling2D\n",
      "[INFO] 120\tConv2D\n",
      "[INFO] 121\tConv2D\n",
      "[INFO] 122\tConv2D\n",
      "[INFO] 123\tConv2D\n",
      "[INFO] 124\tBatchNormalization\n",
      "[INFO] 125\tBatchNormalization\n",
      "[INFO] 126\tBatchNormalization\n",
      "[INFO] 127\tBatchNormalization\n",
      "[INFO] 128\tActivation\n",
      "[INFO] 129\tActivation\n",
      "[INFO] 130\tActivation\n",
      "[INFO] 131\tActivation\n",
      "[INFO] 132\tConcatenate\n",
      "[INFO] 133\tConv2D\n",
      "[INFO] 134\tBatchNormalization\n",
      "[INFO] 135\tActivation\n",
      "[INFO] 136\tConv2D\n",
      "[INFO] 137\tBatchNormalization\n",
      "[INFO] 138\tActivation\n",
      "[INFO] 139\tConv2D\n",
      "[INFO] 140\tConv2D\n",
      "[INFO] 141\tBatchNormalization\n",
      "[INFO] 142\tBatchNormalization\n",
      "[INFO] 143\tActivation\n",
      "[INFO] 144\tActivation\n",
      "[INFO] 145\tConv2D\n",
      "[INFO] 146\tConv2D\n",
      "[INFO] 147\tBatchNormalization\n",
      "[INFO] 148\tBatchNormalization\n",
      "[INFO] 149\tActivation\n",
      "[INFO] 150\tActivation\n",
      "[INFO] 151\tAveragePooling2D\n",
      "[INFO] 152\tConv2D\n",
      "[INFO] 153\tConv2D\n",
      "[INFO] 154\tConv2D\n",
      "[INFO] 155\tConv2D\n",
      "[INFO] 156\tBatchNormalization\n",
      "[INFO] 157\tBatchNormalization\n",
      "[INFO] 158\tBatchNormalization\n",
      "[INFO] 159\tBatchNormalization\n",
      "[INFO] 160\tActivation\n",
      "[INFO] 161\tActivation\n",
      "[INFO] 162\tActivation\n",
      "[INFO] 163\tActivation\n",
      "[INFO] 164\tConcatenate\n",
      "[INFO] 165\tConv2D\n",
      "[INFO] 166\tBatchNormalization\n",
      "[INFO] 167\tActivation\n",
      "[INFO] 168\tConv2D\n",
      "[INFO] 169\tBatchNormalization\n",
      "[INFO] 170\tActivation\n",
      "[INFO] 171\tConv2D\n",
      "[INFO] 172\tConv2D\n",
      "[INFO] 173\tBatchNormalization\n",
      "[INFO] 174\tBatchNormalization\n",
      "[INFO] 175\tActivation\n",
      "[INFO] 176\tActivation\n",
      "[INFO] 177\tConv2D\n",
      "[INFO] 178\tConv2D\n",
      "[INFO] 179\tBatchNormalization\n",
      "[INFO] 180\tBatchNormalization\n",
      "[INFO] 181\tActivation\n",
      "[INFO] 182\tActivation\n",
      "[INFO] 183\tAveragePooling2D\n",
      "[INFO] 184\tConv2D\n",
      "[INFO] 185\tConv2D\n",
      "[INFO] 186\tConv2D\n",
      "[INFO] 187\tConv2D\n",
      "[INFO] 188\tBatchNormalization\n",
      "[INFO] 189\tBatchNormalization\n",
      "[INFO] 190\tBatchNormalization\n",
      "[INFO] 191\tBatchNormalization\n",
      "[INFO] 192\tActivation\n",
      "[INFO] 193\tActivation\n",
      "[INFO] 194\tActivation\n",
      "[INFO] 195\tActivation\n",
      "[INFO] 196\tConcatenate\n",
      "[INFO] 197\tConv2D\n",
      "[INFO] 198\tBatchNormalization\n",
      "[INFO] 199\tActivation\n",
      "[INFO] 200\tConv2D\n",
      "[INFO] 201\tBatchNormalization\n",
      "[INFO] 202\tActivation\n",
      "[INFO] 203\tConv2D\n",
      "[INFO] 204\tConv2D\n",
      "[INFO] 205\tBatchNormalization\n",
      "[INFO] 206\tBatchNormalization\n",
      "[INFO] 207\tActivation\n",
      "[INFO] 208\tActivation\n",
      "[INFO] 209\tConv2D\n",
      "[INFO] 210\tConv2D\n",
      "[INFO] 211\tBatchNormalization\n",
      "[INFO] 212\tBatchNormalization\n",
      "[INFO] 213\tActivation\n",
      "[INFO] 214\tActivation\n",
      "[INFO] 215\tAveragePooling2D\n",
      "[INFO] 216\tConv2D\n",
      "[INFO] 217\tConv2D\n",
      "[INFO] 218\tConv2D\n",
      "[INFO] 219\tConv2D\n",
      "[INFO] 220\tBatchNormalization\n",
      "[INFO] 221\tBatchNormalization\n",
      "[INFO] 222\tBatchNormalization\n",
      "[INFO] 223\tBatchNormalization\n",
      "[INFO] 224\tActivation\n",
      "[INFO] 225\tActivation\n",
      "[INFO] 226\tActivation\n",
      "[INFO] 227\tActivation\n",
      "[INFO] 228\tConcatenate\n",
      "[INFO] 229\tConv2D\n",
      "[INFO] 230\tBatchNormalization\n",
      "[INFO] 231\tActivation\n",
      "[INFO] 232\tConv2D\n",
      "[INFO] 233\tBatchNormalization\n",
      "[INFO] 234\tActivation\n",
      "[INFO] 235\tConv2D\n",
      "[INFO] 236\tConv2D\n",
      "[INFO] 237\tBatchNormalization\n",
      "[INFO] 238\tBatchNormalization\n",
      "[INFO] 239\tActivation\n",
      "[INFO] 240\tActivation\n",
      "[INFO] 241\tConv2D\n",
      "[INFO] 242\tConv2D\n",
      "[INFO] 243\tBatchNormalization\n",
      "[INFO] 244\tBatchNormalization\n",
      "[INFO] 245\tActivation\n",
      "[INFO] 246\tActivation\n",
      "[INFO] 247\tMaxPooling2D\n",
      "[INFO] 248\tConcatenate\n",
      "[INFO] 249\tConv2D\n",
      "[INFO] 250\tBatchNormalization\n",
      "[INFO] 251\tActivation\n",
      "[INFO] 252\tConv2D\n",
      "[INFO] 253\tConv2D\n",
      "[INFO] 254\tBatchNormalization\n",
      "[INFO] 255\tBatchNormalization\n",
      "[INFO] 256\tActivation\n",
      "[INFO] 257\tActivation\n",
      "[INFO] 258\tConv2D\n",
      "[INFO] 259\tConv2D\n",
      "[INFO] 260\tConv2D\n",
      "[INFO] 261\tConv2D\n",
      "[INFO] 262\tAveragePooling2D\n",
      "[INFO] 263\tConv2D\n",
      "[INFO] 264\tBatchNormalization\n",
      "[INFO] 265\tBatchNormalization\n",
      "[INFO] 266\tBatchNormalization\n",
      "[INFO] 267\tBatchNormalization\n",
      "[INFO] 268\tConv2D\n",
      "[INFO] 269\tBatchNormalization\n",
      "[INFO] 270\tActivation\n",
      "[INFO] 271\tActivation\n",
      "[INFO] 272\tActivation\n",
      "[INFO] 273\tActivation\n",
      "[INFO] 274\tBatchNormalization\n",
      "[INFO] 275\tActivation\n",
      "[INFO] 276\tConcatenate\n",
      "[INFO] 277\tConcatenate\n",
      "[INFO] 278\tActivation\n",
      "[INFO] 279\tConcatenate\n",
      "[INFO] 280\tConv2D\n",
      "[INFO] 281\tBatchNormalization\n",
      "[INFO] 282\tActivation\n",
      "[INFO] 283\tConv2D\n",
      "[INFO] 284\tConv2D\n",
      "[INFO] 285\tBatchNormalization\n",
      "[INFO] 286\tBatchNormalization\n",
      "[INFO] 287\tActivation\n",
      "[INFO] 288\tActivation\n",
      "[INFO] 289\tConv2D\n",
      "[INFO] 290\tConv2D\n",
      "[INFO] 291\tConv2D\n",
      "[INFO] 292\tConv2D\n",
      "[INFO] 293\tAveragePooling2D\n",
      "[INFO] 294\tConv2D\n",
      "[INFO] 295\tBatchNormalization\n",
      "[INFO] 296\tBatchNormalization\n",
      "[INFO] 297\tBatchNormalization\n",
      "[INFO] 298\tBatchNormalization\n",
      "[INFO] 299\tConv2D\n",
      "[INFO] 300\tBatchNormalization\n",
      "[INFO] 301\tActivation\n",
      "[INFO] 302\tActivation\n",
      "[INFO] 303\tActivation\n",
      "[INFO] 304\tActivation\n",
      "[INFO] 305\tBatchNormalization\n",
      "[INFO] 306\tActivation\n",
      "[INFO] 307\tConcatenate\n",
      "[INFO] 308\tConcatenate\n",
      "[INFO] 309\tActivation\n",
      "[INFO] 310\tConcatenate\n",
      "[INFO] 311\tFlatten\n",
      "[INFO] 312\tDense\n",
      "[INFO] 313\tDropout\n",
      "[INFO] 314\tDense\n"
     ]
    }
   ],
   "source": [
    "# iterate for all layers in the network and print its' index value\n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(\"[INFO] {}\\t{}\".format(i,layer.__class__.__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unfreeze final CONV layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfrozen\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers[299:]:\n",
    "    layer.trainable = True\n",
    "print('unfrozen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] re-compiling model ...\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "# Build the model from the new\n",
    "print(\"[INFO] re-compiling model ...\")\n",
    "opt = SGD(lr=0.001)\n",
    "# Fine-tuning with a small learning rate\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = opt,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading images...\n",
      "[INFO]: Processed 500/18160\n",
      "[INFO]: Processed 1000/18160\n",
      "[INFO]: Processed 1500/18160\n",
      "[INFO]: Processed 2000/18160\n",
      "[INFO]: Processed 2500/18160\n",
      "[INFO]: Processed 3000/18160\n",
      "[INFO]: Processed 3500/18160\n",
      "[INFO]: Processed 4000/18160\n",
      "[INFO]: Processed 4500/18160\n",
      "[INFO]: Processed 5000/18160\n",
      "[INFO]: Processed 5500/18160\n",
      "[INFO]: Processed 6000/18160\n",
      "[INFO]: Processed 6500/18160\n",
      "[INFO]: Processed 7000/18160\n",
      "[INFO]: Processed 7500/18160\n",
      "[INFO]: Processed 8000/18160\n",
      "[INFO]: Processed 8500/18160\n",
      "[INFO]: Processed 9000/18160\n",
      "[INFO]: Processed 9500/18160\n",
      "[INFO]: Processed 10000/18160\n",
      "[INFO]: Processed 10500/18160\n",
      "[INFO]: Processed 11000/18160\n",
      "[INFO]: Processed 11500/18160\n",
      "[INFO]: Processed 12000/18160\n",
      "[INFO]: Processed 12500/18160\n",
      "[INFO]: Processed 13000/18160\n",
      "[INFO]: Processed 13500/18160\n",
      "[INFO]: Processed 14000/18160\n",
      "[INFO]: Processed 14500/18160\n",
      "[INFO]: Processed 15000/18160\n",
      "[INFO]: Processed 15500/18160\n",
      "[INFO]: Processed 16000/18160\n",
      "[INFO]: Processed 16500/18160\n",
      "[INFO]: Processed 17000/18160\n",
      "[INFO]: Processed 17500/18160\n",
      "[INFO]: Processed 18000/18160\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.image   import ImageDataGenerator\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras .applications import VGG16\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.preprocessors.aspect_aware_preprocessor import AspectAwarePreprocessor\n",
    "from utils.preprocessors.image_to_array_preprocessor import ImageToArrayPreprocessor\n",
    "from utils.io.simple_dataset_loader import SimpleDatasetLoader\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(   rotation_range=30,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "\n",
    "# Load image paths\n",
    "print(\"[INFO] loading images...\")\n",
    "image_paths = list(paths.list_images(dataset_path))\n",
    "\n",
    "# Initial image preprocessing\n",
    "aap = AspectAwarePreprocessor(input_width, input_height)\n",
    "iap= ImageToArrayPreprocessor()\n",
    "\n",
    "#Load image data and perform image data preprocessing\n",
    "sdl = SimpleDatasetLoader(preprocessors=[aap,iap])\n",
    "(data,labels)  = sdl.load(image_paths,verbose=500)\n",
    "data = data.astype(\"float\") / 255.0\n",
    "\n",
    "\n",
    "# train test split\n",
    "(train_x,test_x,train_y,test_y) = train_test_split(data,labels,test_size=0.25,random_state=42)\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "train_y = LabelBinarizer().fit_transform(train_y)\n",
    "test_y = LabelBinarizer().fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints loaded\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', mode='max', \n",
    "save_best_only=True, verbose=1)\n",
    "\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "#load checkpoints if existing\n",
    "import os\n",
    "\n",
    "epochs_done = 54\n",
    "\n",
    "if(os.path.exists(checkpoint_path)):\n",
    "    model.load_weights(checkpoint_path)\n",
    "    num_of_epochs = num_of_epochs - epochs_done\n",
    "    print('checkpoints loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "join() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1ac526821f30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Construct the set of callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfig_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/{}.png\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mjson_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/{}.json\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: join() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "# # Construct the set of callbacks\n",
    "# fig_path = os.path.sep.join([args[\"output\"], \"{}.png\".format(os.getpid())])\n",
    "# json_path = os.path.sep.join([args[\"output\"], \"{}.json\".format(os.getpid())])\n",
    "# callbacks = [TrainingMonitor(fig_path, json_path)]\n",
    "\n",
    "# # train the network\n",
    "# print(\"[INFO]: Training....\") \n",
    "# model.fit(train_x, train_y, validation_data=(test_x, test_y),\n",
    "#  batch_size=64, epochs=100, callbacks=callbacks, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.callbacks.training_monitor import TrainingMonitor\n",
    "import pathlib\n",
    "\n",
    "# Construct the set of callbacks\n",
    "fig_path = os.path.sep.join([str(pathlib.Path().absolute()), \"/{}.png\".format(os.getpid())])\n",
    "json_path = os.path.sep.join([str(pathlib.Path().absolute()), \"/{}.json\".format(os.getpid())])\n",
    "\n",
    "callbacks.append(TrainingMonitor(fig_path, json_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "print(pathlib.Path().absolute());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/46\n",
      "425/425 [==============================] - 377s 888ms/step - loss: 1.6575 - accuracy: 0.3842 - val_loss: 1.5852 - val_accuracy: 0.4396\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.43965, saving model to checkpoints-finetuning.hdf5\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "0.3841625 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-644e761b1759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m              \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m              \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m              callbacks=callbacks)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    258\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m             \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tomato_disease_detection/utils/callbacks/training_monitor.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mseparators\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         default is None and not sort_keys and not kw):\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
      "\u001b[0;32m/usr/lib/python3.5/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" is not JSON serializable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 0.3841625 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "H = model.fit_generator(\n",
    "    aug.flow(train_x,train_y, batch_size = 32),\n",
    "             validation_data = (test_x,test_y),\n",
    "             epochs=num_of_epochs,\n",
    "             steps_per_epoch = len(train_x) //32,\n",
    "             verbose = 1,\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reload model from last best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# import os\n",
    "\n",
    "# model = load_model(warmup_model_path)\n",
    "\n",
    "\n",
    "# if(os.path.exists(checkpoint_path)):\n",
    "#     model.load_weights(checkpoint_path)\n",
    "#     print('weights loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model form disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # from keras.models import load_model\n",
    "\n",
    "# # model_save_path = 'model-final.h5'\n",
    "# # model = load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"[INFO] evaluating after initialization...\")\n",
    "predictions = model.predict(test_x,batch_size=batch_size)\n",
    "\n",
    "print(classification_report(test_y.argmax(axis =1),\n",
    "                            predictions.argmax(axis =1),\n",
    "                            target_names=class_names, \n",
    "                            digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

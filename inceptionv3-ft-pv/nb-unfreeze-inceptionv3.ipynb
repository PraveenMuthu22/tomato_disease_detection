{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Classes:  ['Tomato___Target_Spot', 'Tomato___Late_blight', 'Tomato___Tomato_mosaic_virus', 'Tomato___Leaf_Mold', 'Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Two-spotted_spider_mite', 'Tomato___healthy', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Septoria_leaf_spot']\n10\n"
    }
   ],
   "source": [
    "\n",
    "warmup_model_path = 'model-warmup.h5'\n",
    "model_save_path = 'model-final.h5'\n",
    "dataset_name = 'tomato-dataset'\n",
    "dataset_path = '../datasets/' + dataset_name\n",
    "checkpoint_path = 'checkpoints-finetuning.hdf5'\n",
    "input_width = 224\n",
    "input_height = 224\n",
    "input_depth = 3\n",
    "\n",
    "batch_size = 32\n",
    "num_of_epochs = 100\n",
    "\n",
    "# Get classes\n",
    "import os\n",
    "import re\n",
    "classes = os.listdir(dataset_path)\n",
    "class_names = []\n",
    "\n",
    "for i in classes:\n",
    "    if(re.search(\"Tomato___\", i)):\n",
    "        class_names.append(i)\n",
    "    \n",
    "print('Classes: ', class_names)\n",
    "print(len(class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1.13.1\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "model = tf.keras.models.load_model(\n",
    "    warmup_model_path,\n",
    "    custom_objects=None,\n",
    "    compile=False\n",
    ")\n",
    "\n",
    "# from keras.models import load_model\n",
    "\n",
    "# model = load_model(warmup_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print index of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[INFO] 0\tInputLayer\n[INFO] 1\tConv2D\n[INFO] 2\tBatchNormalizationV1\n[INFO] 3\tActivation\n[INFO] 4\tConv2D\n[INFO] 5\tBatchNormalizationV1\n[INFO] 6\tActivation\n[INFO] 7\tConv2D\n[INFO] 8\tBatchNormalizationV1\n[INFO] 9\tActivation\n[INFO] 10\tMaxPooling2D\n[INFO] 11\tConv2D\n[INFO] 12\tBatchNormalizationV1\n[INFO] 13\tActivation\n[INFO] 14\tConv2D\n[INFO] 15\tBatchNormalizationV1\n[INFO] 16\tActivation\n[INFO] 17\tMaxPooling2D\n[INFO] 18\tConv2D\n[INFO] 19\tBatchNormalizationV1\n[INFO] 20\tActivation\n[INFO] 21\tConv2D\n[INFO] 22\tConv2D\n[INFO] 23\tBatchNormalizationV1\n[INFO] 24\tBatchNormalizationV1\n[INFO] 25\tActivation\n[INFO] 26\tActivation\n[INFO] 27\tAveragePooling2D\n[INFO] 28\tConv2D\n[INFO] 29\tConv2D\n[INFO] 30\tConv2D\n[INFO] 31\tConv2D\n[INFO] 32\tBatchNormalizationV1\n[INFO] 33\tBatchNormalizationV1\n[INFO] 34\tBatchNormalizationV1\n[INFO] 35\tBatchNormalizationV1\n[INFO] 36\tActivation\n[INFO] 37\tActivation\n[INFO] 38\tActivation\n[INFO] 39\tActivation\n[INFO] 40\tConcatenate\n[INFO] 41\tConv2D\n[INFO] 42\tBatchNormalizationV1\n[INFO] 43\tActivation\n[INFO] 44\tConv2D\n[INFO] 45\tConv2D\n[INFO] 46\tBatchNormalizationV1\n[INFO] 47\tBatchNormalizationV1\n[INFO] 48\tActivation\n[INFO] 49\tActivation\n[INFO] 50\tAveragePooling2D\n[INFO] 51\tConv2D\n[INFO] 52\tConv2D\n[INFO] 53\tConv2D\n[INFO] 54\tConv2D\n[INFO] 55\tBatchNormalizationV1\n[INFO] 56\tBatchNormalizationV1\n[INFO] 57\tBatchNormalizationV1\n[INFO] 58\tBatchNormalizationV1\n[INFO] 59\tActivation\n[INFO] 60\tActivation\n[INFO] 61\tActivation\n[INFO] 62\tActivation\n[INFO] 63\tConcatenate\n[INFO] 64\tConv2D\n[INFO] 65\tBatchNormalizationV1\n[INFO] 66\tActivation\n[INFO] 67\tConv2D\n[INFO] 68\tConv2D\n[INFO] 69\tBatchNormalizationV1\n[INFO] 70\tBatchNormalizationV1\n[INFO] 71\tActivation\n[INFO] 72\tActivation\n[INFO] 73\tAveragePooling2D\n[INFO] 74\tConv2D\n[INFO] 75\tConv2D\n[INFO] 76\tConv2D\n[INFO] 77\tConv2D\n[INFO] 78\tBatchNormalizationV1\n[INFO] 79\tBatchNormalizationV1\n[INFO] 80\tBatchNormalizationV1\n[INFO] 81\tBatchNormalizationV1\n[INFO] 82\tActivation\n[INFO] 83\tActivation\n[INFO] 84\tActivation\n[INFO] 85\tActivation\n[INFO] 86\tConcatenate\n[INFO] 87\tConv2D\n[INFO] 88\tBatchNormalizationV1\n[INFO] 89\tActivation\n[INFO] 90\tConv2D\n[INFO] 91\tBatchNormalizationV1\n[INFO] 92\tActivation\n[INFO] 93\tConv2D\n[INFO] 94\tConv2D\n[INFO] 95\tBatchNormalizationV1\n[INFO] 96\tBatchNormalizationV1\n[INFO] 97\tActivation\n[INFO] 98\tActivation\n[INFO] 99\tMaxPooling2D\n[INFO] 100\tConcatenate\n[INFO] 101\tConv2D\n[INFO] 102\tBatchNormalizationV1\n[INFO] 103\tActivation\n[INFO] 104\tConv2D\n[INFO] 105\tBatchNormalizationV1\n[INFO] 106\tActivation\n[INFO] 107\tConv2D\n[INFO] 108\tConv2D\n[INFO] 109\tBatchNormalizationV1\n[INFO] 110\tBatchNormalizationV1\n[INFO] 111\tActivation\n[INFO] 112\tActivation\n[INFO] 113\tConv2D\n[INFO] 114\tConv2D\n[INFO] 115\tBatchNormalizationV1\n[INFO] 116\tBatchNormalizationV1\n[INFO] 117\tActivation\n[INFO] 118\tActivation\n[INFO] 119\tAveragePooling2D\n[INFO] 120\tConv2D\n[INFO] 121\tConv2D\n[INFO] 122\tConv2D\n[INFO] 123\tConv2D\n[INFO] 124\tBatchNormalizationV1\n[INFO] 125\tBatchNormalizationV1\n[INFO] 126\tBatchNormalizationV1\n[INFO] 127\tBatchNormalizationV1\n[INFO] 128\tActivation\n[INFO] 129\tActivation\n[INFO] 130\tActivation\n[INFO] 131\tActivation\n[INFO] 132\tConcatenate\n[INFO] 133\tConv2D\n[INFO] 134\tBatchNormalizationV1\n[INFO] 135\tActivation\n[INFO] 136\tConv2D\n[INFO] 137\tBatchNormalizationV1\n[INFO] 138\tActivation\n[INFO] 139\tConv2D\n[INFO] 140\tConv2D\n[INFO] 141\tBatchNormalizationV1\n[INFO] 142\tBatchNormalizationV1\n[INFO] 143\tActivation\n[INFO] 144\tActivation\n[INFO] 145\tConv2D\n[INFO] 146\tConv2D\n[INFO] 147\tBatchNormalizationV1\n[INFO] 148\tBatchNormalizationV1\n[INFO] 149\tActivation\n[INFO] 150\tActivation\n[INFO] 151\tAveragePooling2D\n[INFO] 152\tConv2D\n[INFO] 153\tConv2D\n[INFO] 154\tConv2D\n[INFO] 155\tConv2D\n[INFO] 156\tBatchNormalizationV1\n[INFO] 157\tBatchNormalizationV1\n[INFO] 158\tBatchNormalizationV1\n[INFO] 159\tBatchNormalizationV1\n[INFO] 160\tActivation\n[INFO] 161\tActivation\n[INFO] 162\tActivation\n[INFO] 163\tActivation\n[INFO] 164\tConcatenate\n[INFO] 165\tConv2D\n[INFO] 166\tBatchNormalizationV1\n[INFO] 167\tActivation\n[INFO] 168\tConv2D\n[INFO] 169\tBatchNormalizationV1\n[INFO] 170\tActivation\n[INFO] 171\tConv2D\n[INFO] 172\tConv2D\n[INFO] 173\tBatchNormalizationV1\n[INFO] 174\tBatchNormalizationV1\n[INFO] 175\tActivation\n[INFO] 176\tActivation\n[INFO] 177\tConv2D\n[INFO] 178\tConv2D\n[INFO] 179\tBatchNormalizationV1\n[INFO] 180\tBatchNormalizationV1\n[INFO] 181\tActivation\n[INFO] 182\tActivation\n[INFO] 183\tAveragePooling2D\n[INFO] 184\tConv2D\n[INFO] 185\tConv2D\n[INFO] 186\tConv2D\n[INFO] 187\tConv2D\n[INFO] 188\tBatchNormalizationV1\n[INFO] 189\tBatchNormalizationV1\n[INFO] 190\tBatchNormalizationV1\n[INFO] 191\tBatchNormalizationV1\n[INFO] 192\tActivation\n[INFO] 193\tActivation\n[INFO] 194\tActivation\n[INFO] 195\tActivation\n[INFO] 196\tConcatenate\n[INFO] 197\tConv2D\n[INFO] 198\tBatchNormalizationV1\n[INFO] 199\tActivation\n[INFO] 200\tConv2D\n[INFO] 201\tBatchNormalizationV1\n[INFO] 202\tActivation\n[INFO] 203\tConv2D\n[INFO] 204\tConv2D\n[INFO] 205\tBatchNormalizationV1\n[INFO] 206\tBatchNormalizationV1\n[INFO] 207\tActivation\n[INFO] 208\tActivation\n[INFO] 209\tConv2D\n[INFO] 210\tConv2D\n[INFO] 211\tBatchNormalizationV1\n[INFO] 212\tBatchNormalizationV1\n[INFO] 213\tActivation\n[INFO] 214\tActivation\n[INFO] 215\tAveragePooling2D\n[INFO] 216\tConv2D\n[INFO] 217\tConv2D\n[INFO] 218\tConv2D\n[INFO] 219\tConv2D\n[INFO] 220\tBatchNormalizationV1\n[INFO] 221\tBatchNormalizationV1\n[INFO] 222\tBatchNormalizationV1\n[INFO] 223\tBatchNormalizationV1\n[INFO] 224\tActivation\n[INFO] 225\tActivation\n[INFO] 226\tActivation\n[INFO] 227\tActivation\n[INFO] 228\tConcatenate\n[INFO] 229\tConv2D\n[INFO] 230\tBatchNormalizationV1\n[INFO] 231\tActivation\n[INFO] 232\tConv2D\n[INFO] 233\tBatchNormalizationV1\n[INFO] 234\tActivation\n[INFO] 235\tConv2D\n[INFO] 236\tConv2D\n[INFO] 237\tBatchNormalizationV1\n[INFO] 238\tBatchNormalizationV1\n[INFO] 239\tActivation\n[INFO] 240\tActivation\n[INFO] 241\tConv2D\n[INFO] 242\tConv2D\n[INFO] 243\tBatchNormalizationV1\n[INFO] 244\tBatchNormalizationV1\n[INFO] 245\tActivation\n[INFO] 246\tActivation\n[INFO] 247\tMaxPooling2D\n[INFO] 248\tConcatenate\n[INFO] 249\tConv2D\n[INFO] 250\tBatchNormalizationV1\n[INFO] 251\tActivation\n[INFO] 252\tConv2D\n[INFO] 253\tConv2D\n[INFO] 254\tBatchNormalizationV1\n[INFO] 255\tBatchNormalizationV1\n[INFO] 256\tActivation\n[INFO] 257\tActivation\n[INFO] 258\tConv2D\n[INFO] 259\tConv2D\n[INFO] 260\tConv2D\n[INFO] 261\tConv2D\n[INFO] 262\tAveragePooling2D\n[INFO] 263\tConv2D\n[INFO] 264\tBatchNormalizationV1\n[INFO] 265\tBatchNormalizationV1\n[INFO] 266\tBatchNormalizationV1\n[INFO] 267\tBatchNormalizationV1\n[INFO] 268\tConv2D\n[INFO] 269\tBatchNormalizationV1\n[INFO] 270\tActivation\n[INFO] 271\tActivation\n[INFO] 272\tActivation\n[INFO] 273\tActivation\n[INFO] 274\tBatchNormalizationV1\n[INFO] 275\tActivation\n[INFO] 276\tConcatenate\n[INFO] 277\tConcatenate\n[INFO] 278\tActivation\n[INFO] 279\tConcatenate\n[INFO] 280\tConv2D\n[INFO] 281\tBatchNormalizationV1\n[INFO] 282\tActivation\n[INFO] 283\tConv2D\n[INFO] 284\tConv2D\n[INFO] 285\tBatchNormalizationV1\n[INFO] 286\tBatchNormalizationV1\n[INFO] 287\tActivation\n[INFO] 288\tActivation\n[INFO] 289\tConv2D\n[INFO] 290\tConv2D\n[INFO] 291\tConv2D\n[INFO] 292\tConv2D\n[INFO] 293\tAveragePooling2D\n[INFO] 294\tConv2D\n[INFO] 295\tBatchNormalizationV1\n[INFO] 296\tBatchNormalizationV1\n[INFO] 297\tBatchNormalizationV1\n[INFO] 298\tBatchNormalizationV1\n[INFO] 299\tConv2D\n[INFO] 300\tBatchNormalizationV1\n[INFO] 301\tActivation\n[INFO] 302\tActivation\n[INFO] 303\tActivation\n[INFO] 304\tActivation\n[INFO] 305\tBatchNormalizationV1\n[INFO] 306\tActivation\n[INFO] 307\tConcatenate\n[INFO] 308\tConcatenate\n[INFO] 309\tActivation\n[INFO] 310\tConcatenate\n[INFO] 311\tFlatten\n[INFO] 312\tDense\n[INFO] 313\tDropout\n[INFO] 314\tDense\n"
    }
   ],
   "source": [
    "# iterate for all layers in the network and print its' index value\n",
    "for (i,layer) in enumerate(model.layers):\n",
    "    print(\"[INFO] {}\\t{}\".format(i,layer.__class__.__name__))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze final CONV layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[299:]:\n",
    "    layer.trainable = True\n",
    "print('unfrozen')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD\n",
    "\n",
    "# Build the model from the new\n",
    "print(\"[INFO] re-compiling model ...\")\n",
    "opt = SGD(lr=0.001)\n",
    "# Fine-tuning with a small learning rate\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = opt,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from keras.preprocessing.image   import ImageDataGenerator\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras .applications import VGG16\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.preprocessors.aspect_aware_preprocessor import AspectAwarePreprocessor\n",
    "from utils.preprocessors.image_to_array_preprocessor import ImageToArrayPreprocessor\n",
    "from utils.io.simple_dataset_loader import SimpleDatasetLoader\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(   rotation_range=30,\n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "\n",
    "# Load image paths\n",
    "print(\"[INFO] loading images...\")\n",
    "image_paths = list(paths.list_images(dataset_path))\n",
    "\n",
    "# Initial image preprocessing\n",
    "aap = AspectAwarePreprocessor(input_width, input_height)\n",
    "iap= ImageToArrayPreprocessor()\n",
    "\n",
    "#Load image data and perform image data preprocessing\n",
    "sdl = SimpleDatasetLoader(preprocessors=[aap,iap])\n",
    "(data,labels)  = sdl.load(image_paths,verbose=500)\n",
    "data = data.astype(\"float\") / 255.0\n",
    "\n",
    "\n",
    "# train test split\n",
    "(train_x,test_x,train_y,test_y) = train_test_split(data,labels,test_size=0.25,random_state=42)\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "train_y = LabelBinarizer().fit_transform(train_y)\n",
    "test_y = LabelBinarizer().fit_transform(test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', mode='max', \n",
    "save_best_only=True, verbose=1)\n",
    "\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "#load checkpoints if existing\n",
    "import os\n",
    "\n",
    "epochs_done = 0\n",
    "\n",
    "if(os.path.exists(checkpoint_path)):\n",
    "    model.load_weights(checkpoint_path)\n",
    "    num_of_epochs = num_of_epochs - epochs_done\n",
    "    print('checkpoints loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = model.fit_generator(\n",
    "    aug.flow(train_x,train_y, batch_size = 32),\n",
    "             validation_data = (test_x,test_y),\n",
    "             epochs=num_of_epochs,\n",
    "             steps_per_epoch = len(train_x) //32,\n",
    "             verbose = 1,\n",
    "             callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model from last best checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import load_model\n",
    "# import os\n",
    "\n",
    "# model = load_model(warmup_model_path)\n",
    "\n",
    "\n",
    "# if(os.path.exists(checkpoint_path)):\n",
    "#     model.load_weights(checkpoint_path)\n",
    "#     print('weights loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model form disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # from keras.models import load_model\n",
    "\n",
    "# # model_save_path = 'model-final.h5'\n",
    "# # model = load_model(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"[INFO] evaluating after initialization...\")\n",
    "predictions = model.predict(test_x,batch_size=batch_size)\n",
    "\n",
    "print(classification_report(test_y.argmax(axis =1),\n",
    "                            predictions.argmax(axis =1),\n",
    "                            target_names=class_names, \n",
    "                            digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_save_path = 'plot-fine-tuning.png'\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, num_of_epochs), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, num_of_epochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, num_of_epochs), H.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(np.arange(0, num_of_epochs), H.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "#save to disk\n",
    "plt.savefig(plot_save_path)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
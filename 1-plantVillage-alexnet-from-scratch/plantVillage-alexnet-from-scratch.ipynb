{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize AlexNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 55, 55, 96)        34944     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 55, 55, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 27, 27, 96)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 17, 17, 256)       2973952   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 6, 6, 384)         885120    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 6, 6, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 4, 4, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 4, 4, 384)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 2, 2, 256)         884992    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 1, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              1052672   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 17)                17017     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 17)                0         \n",
      "=================================================================\n",
      "Total params: 28,054,497\n",
      "Trainable params: 28,054,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.models.alexnet import alexnet\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "model = alexnet()\n",
    "model.summary()\n",
    "plot_model(model, to_file=\"alexnet_model.png\", show_shapes=True)\n",
    "\n",
    "# copile model\n",
    "opt = SGD(lr=0.05)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18160 images loaded\n",
      "[INFO]: Processed 500/18160\n",
      "[INFO]: Processed 1000/18160\n",
      "[INFO]: Processed 1500/18160\n",
      "[INFO]: Processed 2000/18160\n",
      "[INFO]: Processed 2500/18160\n",
      "[INFO]: Processed 3000/18160\n",
      "[INFO]: Processed 3500/18160\n",
      "[INFO]: Processed 4000/18160\n",
      "[INFO]: Processed 4500/18160\n",
      "[INFO]: Processed 5000/18160\n",
      "[INFO]: Processed 5500/18160\n",
      "[INFO]: Processed 6000/18160\n",
      "[INFO]: Processed 6500/18160\n",
      "[INFO]: Processed 7000/18160\n",
      "[INFO]: Processed 7500/18160\n",
      "[INFO]: Processed 8000/18160\n",
      "[INFO]: Processed 8500/18160\n",
      "[INFO]: Processed 9000/18160\n",
      "[INFO]: Processed 9500/18160\n",
      "[INFO]: Processed 10000/18160\n",
      "[INFO]: Processed 10500/18160\n",
      "[INFO]: Processed 11000/18160\n",
      "[INFO]: Processed 11500/18160\n",
      "[INFO]: Processed 12000/18160\n",
      "[INFO]: Processed 12500/18160\n",
      "[INFO]: Processed 13000/18160\n",
      "[INFO]: Processed 13500/18160\n",
      "[INFO]: Processed 14000/18160\n",
      "[INFO]: Processed 14500/18160\n",
      "[INFO]: Processed 15000/18160\n",
      "[INFO]: Processed 15500/18160\n",
      "[INFO]: Processed 16000/18160\n",
      "[INFO]: Processed 16500/18160\n",
      "[INFO]: Processed 17000/18160\n",
      "[INFO]: Processed 17500/18160\n",
      "[INFO]: Processed 18000/18160\n",
      "['Tomato___Bacterial_spot', 'Tomato___Early_blight', 'Tomato___Late_blight', 'Tomato___Leaf_Mold', 'Tomato___Septoria_leaf_spot', 'Tomato___Spider_mites Two-spotted_spider_mite', 'Tomato___Target_Spot', 'Tomato___Tomato_Yellow_Leaf_Curl_Virus', 'Tomato___Tomato_mosaic_virus', 'Tomato___healthy']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from imutils import paths\n",
    "\n",
    "\n",
    "\n",
    "from utils.preprocessors.aspect_aware_preprocessor import AspectAwarePreprocessor\n",
    "from utils.preprocessors.image_to_array_preprocessor import ImageToArrayPreprocessor\n",
    "from utils.io.simple_dataset_loader import SimpleDatasetLoader\n",
    "\n",
    "\n",
    "dataset_path = '../datasets/plantVillage-tomato-mohanty'\n",
    "\n",
    "# Get the list of image names\n",
    "image_paths = list(paths.list_images(dataset_path))\n",
    "\n",
    "print(len(image_paths), 'images loaded')\n",
    "\n",
    "# Get unique classnames\n",
    "classNames = [pt.split(os.path.sep)[-2] for pt in image_paths]\n",
    "classNames = [str(x) for x in np.unique(classNames)]\n",
    "\n",
    "# Preprocessors\n",
    "aap = AspectAwarePreprocessor(227, 227)\n",
    "itap = ImageToArrayPreprocessor()\n",
    "\n",
    "# Add preprocessors to DataSetLoader class\n",
    "sdl = SimpleDatasetLoader(preprocessors=[aap, itap])\n",
    "\n",
    "(data, labels) = sdl.load(image_paths, verbose=500)\n",
    "data = data.astype('float') / 255.0\n",
    "\n",
    "print(classNames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode labels and split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training (75%) and testing (25%) data\n",
    "(train_x, test_x, train_y, test_y) = train_test_split(data, labels,\n",
    "test_size=0.25, random_state=42)\n",
    "\n",
    "# Convert the labels from integers to vectors\n",
    "lb = LabelBinarizer()\n",
    "train_y = lb.fit_transform(train_y)\n",
    "test_y = lb.transform(test_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_9 to have shape (17,) but got array with shape (10,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b387bc271e92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] training network...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m H = model.fit(train_x, train_y, validation_data=(test_x, test_y),\n\u001b[0;32m----> 4\u001b[0;31m               batch_size=32, epochs=100, verbose=1)\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'alexnet_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_9 to have shape (17,) but got array with shape (10,)"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(train_x, train_y, validation_data=(test_x, test_y),\n",
    "              batch_size=32, epochs=100, verbose=1)\n",
    "\n",
    "model.save('alexnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate classfication report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# If one hot encoding is used\n",
    "print(classification_report(test_y, model.predict(test_x),\n",
    "                            target_names=classNames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss and accuracy\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, 100), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, 100), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "plt.savefig(args[\"output\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
